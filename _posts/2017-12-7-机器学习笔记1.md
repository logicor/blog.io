
#1-单变量线性回归linear-regression-with-one-variable



<h4 id="11-模型表示">1.1 模型表示</h4>

<p><img src="http://upload-images.jianshu.io/upload_images/3012260-a6d5768c43fb85c6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="单变量线性回归" title=""></p>

<p>像上述公式，因为只含有一个特征/输入变量，因此这样的问题叫作单变量线性回归问题。</p>

<p>例子如下： <br>
<img src="http://upload-images.jianshu.io/upload_images/3012260-63e145fce22cc011.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="回归函数图示" title=""></p>

<p>单变量线性方程，就是我们初中就学的一元一次函数。 <br>
当然啦，除了这个模型之外，我们还有很多其他的线性模型，比如指数模型、对数模型等等，除了线性模型之外，还有非线性模型，有这么多的模型，其目的就是在于更好的拟合训练集的数据，以使得预测率更高。</p>

<p>以下是对模型的具体定义：</p>

<p><img src="http://upload-images.jianshu.io/upload_images/3012260-f3f61877293c3449.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="回归图示" title=""></p>



<h3 id="2-代价函数cost-function"><a name="t1"></a>2. 代价函数(Cost Function)</h3>

<blockquote>
  <p>代价函数就是为了就是找到目的函数的最优解。</p>
</blockquote>

<p>因为在一个训练集中，有无数个模型（一元一次函数），我们需要找到最拟合这个训练集的一个函数，所以就引入了代价函数，用来找到那个最好的模型。</p>



<h4 id="21公式表示">2.1公式表示</h4>

<p><img src="http://upload-images.jianshu.io/upload_images/3012260-191197cdb45a545a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="代价函数" title=""></p>

<p>上述是平方误差代价函数，这也是常用到的代价函数，它通过目的函数跟各个实际值的误差平方建立新的函数。为了使这个值不受个别极端数据影响而产生巨大波动，采用类似方差再取二分之一的方式来减小个别数据的影响。</p>

<p><img src="http://upload-images.jianshu.io/upload_images/3012260-54e42067c1836645.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="平方误差代价函数图示" title=""></p>



<h4 id="22-代价函数的直观理解①">2.2 代价函数的直观理解①</h4>

<p>最优解即为代价函数的最小值，根据以上公式多次计算可得到代价函数的图像： <br>
<img src="http://upload-images.jianshu.io/upload_images/3012260-fb49ceb8fc72d23a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="代价函数图示" title=""> <br>
可以看到该代价函数的确有最小值，这里恰好是横坐标为1的时候。</p>



<h4 id="23-代价函数的直观理解②">2.3 代价函数的直观理解②</h4>

<p>如果有更多参数，就会更为复杂，两个参数的时候就已经是三维图像了：  <br>
<img src="http://upload-images.jianshu.io/upload_images/3012260-1571cf5726674c91.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="代价函数图示2" title=""></p>



<h3 id="3-梯度下降算法gradient-descent"><a name="t2"></a>3. 梯度下降算法(Gradient Descent)</h3>

<blockquote>
  <p>梯度下降是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出代价函数J(θ0,θ1) 的最小值。</p>
</blockquote>

<p>个人理解，代价函数是分析模型与实际训练集之间的误差，而梯度下降算法的作用，就是找出那个误差最小的代价函数。</p>



<h4 id="算法思想">算法思想</h4>

<p><img src="http://upload-images.jianshu.io/upload_images/3012260-4483dae7b0bcfdb7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="算法思想" title=""> <br>
- 从参数的某一个（组）值开始，比如从θ0=0和θ1=0开始 <br>
- 保持该（组）值持续减小，如果是一组值就要保证他们<strong>同步更新</strong>，直到找到我们希望找到的最小值</p>

<p>我们要找到一条最快下山的路径，我们走的每一步大小就是α 。 <br>
<img src="http://upload-images.jianshu.io/upload_images/3012260-be695f829cf7ec8a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="梯度下降图示1" title=""></p>

<p>如果在不同的起点，最后到达的最低点也会不一样。 <br>
<img src="http://upload-images.jianshu.io/upload_images/3012260-ac8e71c4939bc4b0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="梯度下降图示2" title=""></p>



<h4 id="31批量梯度下降batch-gradient-descent">3.1批量梯度下降(batch gradient descent)</h4>

<p><img src="http://upload-images.jianshu.io/upload_images/3012260-fb2e4da37f784b64.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="批量梯度下降" title=""></p>

<ul>
<li>α：学习速率，决定我们让代价函数下降程度最大的方向迈出的步子有多大</li>
</ul>



<h5 id="311-同步更新simultaneous-update">3.1.1 同步更新(Simultaneous update)</h5>

<p>在梯度下降算法中，我们需要更新θ0,θ1，实现梯度下降算法的微妙之处是，在这个表达式中，如果你要更新这个等式，你需要<strong>同时</strong>更新。</p>

<p><img src="http://upload-images.jianshu.io/upload_images/3012260-b9ac6f6c8e4ae4f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="同步更新公式" title=""></p>



<h5 id="312-梯度下降算法理解">3.1.2 梯度下降算法理解</h5>

<p>如果 α 太大，那么梯度下降法可能会越过最低点，甚至可能无法收敛，下一次迭代又移动了一大步，越过一次，又越过一次，一次次越过最低点，直到你发现实际上离最低点越来越远，所以，如果 α 太大，它会导致无法收敛，甚至发散。</p>

<p><img src="http://upload-images.jianshu.io/upload_images/3012260-04be937369aee434.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="对α的理解" title=""></p>



<h5 id="解决方法乘偏导数">解决方法——乘偏导数</h5>

<p><img src="http://upload-images.jianshu.io/upload_images/3012260-d709146129e3cba8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="批量梯度下降直观图" title=""></p>

<p>首先初始化我的梯度下降算法，在那个品红色的点初始化，如果 <br>
我更新一步梯度下降，随着我接近最低点，我的导数越来越接近零，所以，梯度下降一步后，新的导数会变小一点点。然后我想再梯度下降一步，在这个绿点，我自然会用一个稍微跟刚才在那个品红点时比，再小一点的一步，到了新的红色点，更接近全局最低点了，因此这点的导数会比在绿点时更小。所 以，我再进行一步梯度下降时，我的导数项是更小的，θ1更新的幅度就会更小。所以随着梯度下降法的运行，你移动的幅度会自动变得越来越小，直到最终移动幅度非常小，你会发现，已经收敛到局部极小值。</p>



<h5 id="313-线性回归的批量梯度下降">3.1.3 线性回归的批量梯度下降</h5>

<p>偏导数求解推导过程</p>

<p><img src="http://upload-images.jianshu.io/upload_images/3012260-d2614d40be9bd3bc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="偏导数求解推导过程" title=""></p>



<h5 id="批量梯度下降方程">批量梯度下降方程</h5>

<p>通过上面几条公式的整合，最终得出以下公式 <br>
<img src="http://upload-images.jianshu.io/upload_images/3012260-14960fdce63e7804.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="线性回归方程" title=""></p>



<h4 id="4-线性代数基础">4. 线性代数基础</h4>

<p>个人现在认为，线性代数的作用主要是为了方便操作训练集。</p>



<h5 id="41-矩阵的定义">4.1 矩阵的定义</h5>

<p>横为行，竖为列，表示方法一般是R^(m*n) <br>
<img src="http://upload-images.jianshu.io/upload_images/3012260-10e03c4ad5ff9764.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="矩阵的定义" title=""></p>

<p>寻找某个矩阵元素 <br>
<img src="http://upload-images.jianshu.io/upload_images/3012260-7869e41523b65eb1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="某个矩阵元素" title=""></p>



<h5 id="42-矩阵加法matrix-addition">4.2 矩阵加法(Matrix Addition)</h5>

<p>同一个位置的矩阵元素相加，得到新的矩阵 <br>
<img src="http://upload-images.jianshu.io/upload_images/3012260-5be88f7936d62d83.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="矩阵加法" title=""></p>



<h5 id="43-矩阵乘法scalar-multiplication">4.3 矩阵乘法(Scalar Multiplication)</h5>

<p>将值与矩阵每个元素相乘，得到新的矩阵 <br>
<img src="http://upload-images.jianshu.io/upload_images/3012260-15bc81386707f6b4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="矩阵乘法" title=""></p>



<h5 id="44-矩阵的组合运算combination-of-operands">4.4 矩阵的组合运算(Combination of Operands)</h5>

<p>将矩阵加减法和乘除法结合起来，道理都一样 <br>
<img src="http://upload-images.jianshu.io/upload_images/3012260-833eabb3c17f0db9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="矩阵的组合运算" title=""></p>



<h5 id="45-两个矩阵相乘">4.5 两个矩阵相乘</h5>

<p>A矩阵的行 乘 B矩阵的列 得到新矩阵 y 。 <br>
<img src="http://upload-images.jianshu.io/upload_images/3012260-369c408f575cfe44.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="两个矩阵相乘1" title=""></p>

<p><img src="http://upload-images.jianshu.io/upload_images/3012260-27438c405289a620.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="两个矩阵相乘2" title=""></p>



<h5 id="46-矩阵应用到梯度下降算法实例">4.6 矩阵应用到梯度下降算法实例</h5>

<p>把训练集做成一个矩阵，把线性回归方程做成另外一个矩阵，将两个矩阵相乘，最后就能得出一个新的矩阵。 <br>
<img src="http://upload-images.jianshu.io/upload_images/3012260-21341fea26117c6e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="矩阵应用到梯度下降算法实例图示" title=""></p>



<h5 id="47-单位矩阵">4.7 单位矩阵</h5>

<blockquote>
  <p>在矩阵的乘法中，有一种矩阵起着特殊的作用，如同数的乘法中的1,这种矩阵被称为单位矩阵．它是个方阵，从左上角到右下角的对角线（称为主对角线）上的元素均为1。除此以外全都为0。</p>
</blockquote>

<p><img src="http://upload-images.jianshu.io/upload_images/3012260-d0a6194ff1a63663.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="单位矩阵" title=""></p>

<p>除0矩阵外，任何矩阵乘单位矩阵都等于它本身。</p>

<p><img src="http://upload-images.jianshu.io/upload_images/3012260-7bf3071433710740.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="单位矩阵运算" title=""></p>



<h5 id="48-逆矩阵">4.8 逆矩阵</h5>

<p><img src="http://upload-images.jianshu.io/upload_images/3012260-2225b83ba8fbe983.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="定义" title=""></p>

<p>用octave求得逆矩阵：pinv()函数</p>

<p><img src="http://upload-images.jianshu.io/upload_images/3012260-8b37914823b78864.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="octave求得逆矩阵" title=""></p>                